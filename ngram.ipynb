{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# N-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some global variables for word preprocessing and ngrams\n",
    "START_CHAR = '<s>'\n",
    "END_CHAR = '<e>'\n",
    "UNK = '<u>'\n",
    "PUNCTUATION = '\".,;:+*`´¨!?¡¿&%()/' + \"'\"\n",
    "N = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess(sentence, n):\n",
    "    '''\n",
    "    Remove whitespaces and punctuation, turn into lowercase, add necessary starting and ending\n",
    "    dummy words. \n",
    "    Takes:\n",
    "        - sentence, a list of strings\n",
    "        - n, an int indicating the n-grams\n",
    "    \n",
    "    Returns:\n",
    "        - words, a list of words stripped of punctuation and with additional characters\n",
    "        to build ngram model\n",
    "    '''\n",
    "    words = ' '.join(sentence).encode('utf-8').lower().strip().split()\n",
    "    words = [START_CHAR]*(n-1) + words + [END_CHAR]\n",
    "    words = [word.strip(PUNCTUATION) for word in words]\n",
    "    words = [word for word in words if len(word) != 0]\n",
    "\n",
    "    return words\n",
    "\n",
    "def init_dict():\n",
    "    return {'_wordcount': 0}\n",
    "\n",
    "def add_words(d, words, n):\n",
    "    '''\n",
    "    Modifies d, a dictionary with counts of ngrams, on site.\n",
    "    Takes:\n",
    "        - d, a dictionary where words will be added\n",
    "        - words, a list of strings representing words\n",
    "        - n, size of ngrams\n",
    "    '''\n",
    "    for i in range(len(words) - n):\n",
    "        prefix = ' '.join(words[i:i + n])\n",
    "        value  = words[i + n]\n",
    "        d['_wordcount'] += 1\n",
    "        if prefix not in d:\n",
    "            d[prefix] = init_dict()\n",
    "        temp = d[prefix]\n",
    "        temp['_wordcount'] += 1\n",
    "        if value not in temp:\n",
    "            temp[value] = 0\n",
    "        temp[value] += 1\n",
    "        \n",
    "def calculate_probability(words, corpus, n):\n",
    "    '''\n",
    "    Takes:\n",
    "        words, a preprocessed list of words, according to n\n",
    "        corpus, a dictionary with counts corresponding to a set of sentences\n",
    "        n, an int indicating size of ngrams\n",
    "        \n",
    "    Returns:\n",
    "        probability (float) of a word in the model\n",
    "    '''\n",
    "    logp = 0\n",
    "    for i in range(len(words) - n + 1):\n",
    "        \n",
    "        if n == 1:\n",
    "            if words[i] not in corpus['']: \n",
    "                temp_p = 1.0 / corpus['']['_wordcount'] \n",
    "            else:\n",
    "                temp_p = 1.0 * corpus[''][words[i]] / corpus['']['_wordcount'] \n",
    "                \n",
    "        else:\n",
    "            prefix = ' '.join(words[i:i + n - 1])\n",
    "            if (prefix not in corpus) or (words[i + n - 1] not in corpus[prefix]): \n",
    "                temp_p = 1.0 / corpus['_wordcount'] \n",
    "            else:\n",
    "                temp_p = 1.0 * corpus[prefix][words[i + n - 1]] / corpus[prefix]['_wordcount']\n",
    "        logp += math.log(temp_p)\n",
    "    \n",
    "    return math.exp(logp)\n",
    "\n",
    "def linear_smoothing(lambdas, words_list, model_list, n):\n",
    "    '''\n",
    "    Takes:\n",
    "        - lambdas, a list of float values to weight probabilities\n",
    "        - words_list, a list of lists of words with START and END characters \n",
    "        - model_list, a list of models with ngram counts\n",
    "        - n, an int indicating ngram size\n",
    "    \n",
    "    Returns:\n",
    "        rv, a lambda-weighted probability of finding a word in the model_list\n",
    "    '''\n",
    "    rv = 0\n",
    "    for l, words, model, i in zip(lambdas, words_list, model_list, range(n)):\n",
    "        rv += l*calculate_probability(words, model, i+1)\n",
    "\n",
    "    return rv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Separate holdout data. Split remaining into train and test sets. \n",
    "class BrownData(object):\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "        \n",
    "    def save_data_by_genre(self, train, test, holdout, genre):\n",
    "        self.data[genre] = {'train': train, 'test': test, 'holdout': holdout}  \n",
    "        \n",
    "    def split(self):\n",
    "        '''\n",
    "        Splits data into train, test, and holdout sets. Also modifies\n",
    "        the training data to include <UNK> character to correct for unknown words.\n",
    "        '''\n",
    "        for genre in brown.categories():\n",
    "            split = train_test_split(brown.sents(categories=genre))\n",
    "            working, holdout = split[0], split[1]\n",
    "            train, test = train_test_split(working)\n",
    "            self.replace_unk(train)\n",
    "            self.save_data_by_genre(train, test, holdout, genre)\n",
    "            \n",
    "    def replace_unk(self, train):\n",
    "        '''\n",
    "        Takes:\n",
    "            - train, a list of lists of strings\n",
    "        '''\n",
    "        words = []\n",
    "        for i in range(len(train)):\n",
    "            for j in range(len(train[i])):\n",
    "                if train[i][j] not in words:\n",
    "                    words.append(train[i][j])\n",
    "                    train[i][j] = UNK                   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NGramClassifier(object):\n",
    "    \n",
    "    def __init__(self, lambda1, lambda2, lambda3):\n",
    "        #assert lambda1 + lambda2 + lambda3 == 1\n",
    "        self.lambda1, self.lambda2, self.lambda3 = lambda1, lambda2, lambda3\n",
    "        self._unigram, self._bigram, self._trigram = {}, {}, {}\n",
    "        self.genres = set()\n",
    "    \n",
    "    @property\n",
    "    def unigrams(self):\n",
    "        return self._unigram\n",
    "    \n",
    "    @property\n",
    "    def bigrams(self):\n",
    "        return self._bigram\n",
    "    \n",
    "    @property\n",
    "    def trigrams(self):\n",
    "        return self._trigram\n",
    "    \n",
    "    @property\n",
    "    def lambdas(self):\n",
    "        return [self.lambda1, self.lambda2, self.lambda3]\n",
    "    \n",
    "    def modify_lambdas(self, l1, l2, l3):\n",
    "        self.lambda1, self.lambda2, self.lambda3 = l1, l2, l3\n",
    "    \n",
    "    def build_ngram_model(self, n, list_of_sentences):\n",
    "        '''\n",
    "        Takes:\n",
    "            - n, an int indicating number in n-grams\n",
    "            - list_of_sentences, a list of strings with sentences\n",
    "            - genre, a string indicating the genre\n",
    "            \n",
    "        Returns:\n",
    "            - ngram, \n",
    "        '''\n",
    "        model = init_dict()\n",
    "        for sentence in list_of_sentences:\n",
    "            words = preprocess(sentence, n)\n",
    "            add_words(model, words, n-1)\n",
    "            \n",
    "        return model\n",
    "              \n",
    "    def train(self, list_of_sentences, genre):\n",
    "        '''\n",
    "        Takes:\n",
    "            - list_of_sentences, a list of lists of strings [[...],[...],...,[...]]\n",
    "            - genre, a string indicating the genre of interest\n",
    "        '''\n",
    "        self.genres.add(genre)\n",
    "        self._unigram[genre] = self.build_ngram_model(1, list_of_sentences)\n",
    "        self._bigram[genre] = self.build_ngram_model(2, list_of_sentences)\n",
    "        self._trigram[genre] = self.build_ngram_model(3, list_of_sentences)\n",
    "    \n",
    "    def call_model_list_by_genre(self, genre):\n",
    "        return [self.unigrams[genre], self.bigrams[genre], self.trigrams[genre]]\n",
    "        \n",
    "    def predict_one_sentence(self, sentence):\n",
    "        '''\n",
    "        Takes:\n",
    "            - sentence, a list of strings (words)\n",
    "            \n",
    "        Returns:\n",
    "            - results, a dictionary where the key, value pairs are genres, likelihood of\n",
    "            text being of such genre\n",
    "        '''\n",
    "        lambdas = self.lambdas\n",
    "        words_list = [preprocess(sentence, n+1) for n in range(N)]\n",
    "        results = {}\n",
    "        for genre in self.genres:\n",
    "            model_list = self.call_model_list_by_genre(genre)\n",
    "            results[genre] = linear_smoothing(lambdas, words_list, model_list, N)\n",
    "        return results\n",
    "    \n",
    "    def predict(self, list_of_sentences):\n",
    "        '''\n",
    "        Takes:\n",
    "            - list_of_sentences, a list of lists of strings [[...],[...],...,[...]]\n",
    "        \n",
    "        Returns:\n",
    "            - rv, a list of predicted genres (strings) \n",
    "        '''\n",
    "        rv = []\n",
    "        for sentence in list_of_sentences:\n",
    "            results = self.predict_one_sentence(sentence)\n",
    "            max_cat = 0\n",
    "            max_val = 0\n",
    "            for key, value in results.items():\n",
    "                if value > max_val:\n",
    "                    max_val = value\n",
    "                    max_cat = key\n",
    "            #if max_cat == 0: print(results)\n",
    "            rv.append(max_cat)\n",
    "            \n",
    "        return rv\n",
    "    \n",
    "    # results table\n",
    "    def get_labeled_predictions(self, data):\n",
    "        '''\n",
    "        Takes:\n",
    "            - data, a dictionary with train, test, and holdout data\n",
    "            \n",
    "        Returns:\n",
    "            - rv, predicted labels for test data\n",
    "        '''\n",
    "        rv = pd.DataFrame(columns=['prediction', 'label'])\n",
    "        for genre in self.genres:\n",
    "            test = data.data[genre]['test']\n",
    "            results = self.predict(test)\n",
    "            labels = [genre]*len(results)\n",
    "            temp = pd.DataFrame({'prediction': results, 'label': labels})\n",
    "            rv = rv.append(temp)\n",
    "\n",
    "        return rv\n",
    "\n",
    "    #get rates from results table \n",
    "    def get_metrics(self, predictions):\n",
    "        '''\n",
    "        Takes:\n",
    "            - predictions, a list of labels\n",
    "        \n",
    "        Returns:\n",
    "            - tp, fn, fp, fn: true positive, false negative, false positive, and false negative\n",
    "            rates\n",
    "        '''\n",
    "        tp, fn, fp, tn = {}, {}, {}, {}\n",
    "        for genre in self.genres:\n",
    "            g = predictions[predictions['label'] == genre]\n",
    "            tp[genre] = g[g['prediction'] == genre].label.count()\n",
    "            fn[genre] = g[g['prediction'] != genre].label.count()\n",
    "            for other_genre in self.genres:\n",
    "                if other_genre != genre:\n",
    "                    og = predictions[predictions['label'] == other_genre]\n",
    "                    fp[genre] = og[og['prediction'] == genre].label.count()\n",
    "                    tn[genre] = og[og['prediction'] != genre].label.count()\n",
    "\n",
    "        return tp, fn, fp, fn\n",
    "\n",
    "    def micro_avg_precision(self, tp, fp):\n",
    "        tp_sum = 0\n",
    "        fp_sum = 0\n",
    "        for genre in self.genres:\n",
    "            tp_sum += tp[genre]\n",
    "            fp_sum += fp[genre]\n",
    "\n",
    "        return 1.0 * tp_sum / (tp_sum + fp_sum)\n",
    "\n",
    "    def micro_avg_recall(self, tp, fn):\n",
    "        tp_sum = 0\n",
    "        fn_sum = 0\n",
    "        for genre in self.genres:\n",
    "            tp_sum += tp[genre]\n",
    "            fn_sum += fn[genre]\n",
    "\n",
    "        return 1.0 * tp_sum / (tp_sum + fn_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(clf, data, l1 = 0, l2=0.5, l3=0.5):\n",
    "    '''\n",
    "    Takes:\n",
    "        - data, a dictionary with train, test, and holdout data\n",
    "        - l1, l2, l3, lambdas (floats) to weight ngram models\n",
    "    '''\n",
    "    clf.modify_lambdas(l1, l2, l3)\n",
    "    labeled_preds = clf.get_labeled_predictions(data)\n",
    "    tp, fn, fp, fn = clf.get_metrics(labeled_preds)\n",
    "    p = clf.micro_avg_precision(tp, fp)\n",
    "    r = clf.micro_avg_recall(tp, fn)\n",
    "    \n",
    "    return 2*p*r/(p+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do once, because it takes time to load all the words into the dictionary.\n",
    "brown_data = BrownData()\n",
    "brown_data.split()\n",
    "\n",
    "# Create and train a classifier, do once.\n",
    "clf = NGramClassifier(0, 0.5, 0.5) \n",
    "for category in brown_data.data.keys():\n",
    "    clf.train(brown_data.data[category]['train'], category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4029676444322319"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(clf, brown_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0... \n",
      "(0.1701093560145808, [0.0, 0.0, 1.0])\n",
      "(0.4012103706760195, [0.0, 0.1, 0.9])\n",
      "(0.4016500515641114, [0.0, 0.2, 0.8])\n",
      "(0.4020894906866452, [0.0, 0.30000000000000004, 0.7])\n",
      "4... \n",
      "(0.4029676444322319, [0.0, 0.4, 0.6])\n",
      "(0.40346177622089435, [0.0, 0.6000000000000001, 0.3999999999999999])\n",
      "(0.4036810658608613, [0.0, 0.7000000000000001, 0.29999999999999993])\n",
      "8... \n",
      "(0.4039557722683882, [0.0, 0.9, 0.09999999999999998])\n",
      "0... \n",
      "4... \n",
      "8... \n",
      "0... \n",
      "4... \n",
      "8... \n",
      "0... \n",
      "4... \n",
      "8... \n",
      "0... \n",
      "4... \n",
      "8... \n",
      "0... \n",
      "4... \n",
      "8... \n",
      "0... \n",
      "4... \n",
      "8... \n",
      "0... \n",
      "4... \n",
      "8... \n",
      "0... \n",
      "4... \n",
      "8... \n",
      "0... \n",
      "4... \n",
      "8... \n"
     ]
    }
   ],
   "source": [
    "max_val = 0\n",
    "max_weights = []\n",
    "for j in range(10):\n",
    "    for i in range(10):\n",
    "        if i%4 == 0 : print('{}... '.format(i))\n",
    "        l1, l2 = 0.1 * j, 0.1 * i\n",
    "        l3 = 1 - l1 - l2\n",
    "        f1 = test(clf, brown_data, l1, l2, l3)\n",
    "        if f1 > max_val:\n",
    "            max_val = f1\n",
    "            max_weights = [l1, l2, l3]\n",
    "            print(max_val, max_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
